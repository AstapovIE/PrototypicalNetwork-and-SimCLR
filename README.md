# PrototypicalNetwork-and-SimCLR
[DL Project MIPT] Prototypical Network on the Omniglot and SimCLR with resnet18 on Downstream task
 
Работа была выполнена в рамках домашнего задания курса Машинного Обучения в МФТИ. Полная постановка задачи [тут](https://github.com/ml-dafe/ml_mipt_dafe/tree/main/08_Self_Supervision/homework)

**Prototypical Networks** были введены Snell et al. в 2017 г. (https://arxiv.org/abs/1703.05175). Они начали с уже существующей архитектуры под названием Matching Networks, представленной в статье (https://arxiv.org/abs/1606.04080). Обе сети являются частью более широкого семейства алгоритмов, называемых Metric Learning Algorithms. и успех этих сетей основан на их способности понимать отношения сходства между выборками.

«Наш подход основан на идее, что существует вложение, в котором точки группируются вокруг одного прототипа. репрезентация для каждого класса», — заявляют авторы оригинальной статьи Prototype Networks for Few-shot Learning)

Другими словами, существует математическое представление изображений, называемое embedding/latent space, в которых изображения одного класса собираются в кластеры. Основное преимущество работы в этом пространстве заключается в том, что два изображения, которые выглядят одинаково, будут близки друг к другу. и два совершенно разных изображения будут далеко друг от друга. Здесь термин «близко» относится к метрике расстояния, которую необходимо определить. Обычно берется косинусное или евклидово расстояние.
**Ключевая идея ProtoNet - классификация через сравнение с прототипами классов в пространстве эмбеддингов.**

## Проделанная работа:
1. Скачивание и чтение данных Omniglot
2. Реализация **Энкодера** согласно архитектуре, где каждый модуль (блок) состоит из:
  * convolutional layer
  * batch normalization
  * ReLu activation function
  * 2x2 max pooling layer.
3. Прямой проход (forward pass) модели ProtoNet (прототипная сеть) для few-shot обучения. Основные шаги:

 - Подготовка данных
 - Получение эмбеддингов
 - Разделение на support и query
 - Вычисление прототипов
 - Вычисление расстояний
 - Предсказание и вычисление потерь
 - Возврат результатов

4. Функция обучение + логирование обучения с помощью wandb. 
5. Визуализация предсказаний на тестовой выборке.
6. SimCLR для набора данных CIFAR10:

### **Что такое метод SimCLR?**

**SimCLR (Simple Framework for Contrastive Learning of Visual Representations)** — это метод *самообучения* (self-supervised learning), в котором модель учится на **неразмеченных данных** так, чтобы затем её эмбеддинги (представления) можно было использовать для других задач, например, классификации.

Суть:

* Берётся одно изображение и **дважды аугментируется** (например, кроп, флип, цветовые искажения).
* Эти две версии считаются **положительной парой**.
* Остальные изображения в батче — **отрицательные примеры**.
* **Цель** — минимизировать расстояние между эмбеддингами положительной пары и максимизировать между отрицательными (через contrastive loss — чаще **NT-Xent Loss**).
* После обучения: энкодер фиксируют, и используют его для downstream-задач (например, классификации).


### **Реализации SimCLR + downstream задачи**

#### Этап I: Подготовка данных

*  Скачать CIFAR-10
*  Сделать **аугментации**

#### Этап II: Обучение SimCLR

* Взять `resnet18` без последнего `fc` слоя
* Добавить `projection head` — MLP на выходе (`Linear -> ReLU -> Linear`)
* Использовать **NT-Xent Loss** 
* Обучать энкодер на неразмеченных изображениях через контрастивное обучение

#### Этап III: Downstream задача (классификация)

* Взять фиксированный (замороженный) `encoder` после SimCLR
* Подключить новый `classifier head` (например, `Linear(embedding_dim → 10)`)
* Обучить только классификатор на размеченных `X_test`, `y_test`

#### Этап IV: Базовая модель (без SimCLR)

* Повторить классификацию, но обучить `encoder + classifier` с нуля


#### Этап V: Сравнение и визуализация**

#### Что сравниваем:

* Графики accuracy/loss по эпохам:

  * Классификатор **с SimCLR (замороженный encoder)**
  * Классификатор **без SimCLR (всё учим с нуля)**

## Распределение задач внутри команды:
Астапов Илья
  * план работы и оформление отчета в GitHub о проделанной работе
  * 4-5 пункты из проделанной работы
Балала Олег
  * 1-3 пункты из проделанной работы
Панчишин Максим
  * 6 пункт из проделанной работы
